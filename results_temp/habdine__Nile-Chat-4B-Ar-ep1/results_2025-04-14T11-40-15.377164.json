{
  "results": {
    "egyptian_summarization": {
      " ": " ",
      "alias": "egyptian_summarization"
    },
    "summarization_": {
      "alias": " - summarization_",
      "rouge1,STRIP_ANSWER": 0.0,
      "rouge1_stderr,STRIP_ANSWER": "N/A",
      "rouge2,STRIP_ANSWER": 0.0,
      "rouge2_stderr,STRIP_ANSWER": "N/A",
      "rougeL,STRIP_ANSWER": 0.0,
      "rougeL_stderr,STRIP_ANSWER": "N/A",
      "rougeLsum,STRIP_ANSWER": 0.0,
      "rougeLsum_stderr,STRIP_ANSWER": "N/A",
      "bert,STRIP_ANSWER": 0.5526278614997864,
      "bert_stderr,STRIP_ANSWER": "N/A",
      "chrf,STRIP_ANSWER": 22.024646772390298,
      "chrf_stderr,STRIP_ANSWER": 0.32774324436816427
    }
  },
  "group_subtasks": {
    "egyptian_summarization": [
      "summarization_"
    ]
  },
  "configs": {
    "summarization_": {
      "task": "summarization_",
      "tag": [
        "egyptian_summarization_task"
      ],
      "dataset_path": "MBZUAI-Paris/EgyptianBench",
      "dataset_name": "summarization",
      "test_split": "test",
      "doc_to_text": "def doc_to_text(doc):\n    doc_text = doc[\"messages\"][0][\"content\"]\n    return doc_text\n",
      "doc_to_target": "def doc_to_target(doc):\n    return doc[\"messages\"][1][\"content\"]\n",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "def rouge1(items):\n    return items\n",
          "aggregation": "def agg_rouge1(items):\n    rouge = evaluate.load(\"rouge\")\n    predictions, references = zip(*items)\n    return rouge.compute(predictions=predictions, references=references)[\"rouge1\"]\n",
          "higher_is_better": true
        },
        {
          "metric": "def rouge2(items):\n    return items\n",
          "aggregation": "def agg_rouge2(items):\n    rouge = evaluate.load(\"rouge\")\n    predictions, references = zip(*items)\n    return rouge.compute(predictions=predictions, references=references)[\"rouge2\"]\n",
          "higher_is_better": true
        },
        {
          "metric": "def rougeL(items):\n    return items\n",
          "aggregation": "def agg_rougel(items):\n    rouge = evaluate.load(\"rouge\")\n    predictions, references = zip(*items)\n    return rouge.compute(predictions=predictions, references=references)[\"rougeL\"]\n",
          "higher_is_better": true
        },
        {
          "metric": "def rougeLsum(items):\n    return items\n",
          "aggregation": "def agg_rougelsum(items):\n    rouge = evaluate.load(\"rouge\")\n    predictions, references = zip(*items)\n    return rouge.compute(predictions=predictions, references=references)[\"rougeLsum\"]\n",
          "higher_is_better": true
        },
        {
          "metric": "def bert(items):\n    return items\n",
          "aggregation": "def egybert(items):\n    bert_model = 'google-bert/bert-base-multilingual-cased'\n    bert_score = evaluate.load(\"bertscore\")\n    predictions, references = zip(*items)\n    bert = bert_score.compute(predictions=predictions, references=references, model_type=bert_model, num_layers=12)\n    return Average(bert['f1'])\n",
          "higher_is_better": true
        },
        {
          "metric": "chrf",
          "aggregation": "chrf",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "<end_of_turn>",
          "<eos>",
          "</s>",
          "<|end_of_text|>",
          "<|eot_id|>",
          "<|endoftext|>"
        ],
        "do_sample": false,
        "temperature": 0.0,
        "max_new_tokens": 128
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "STRIP_ANSWER",
          "filter": [
            {
              "function": "strip"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    }
  },
  "versions": {
    "summarization_": 1.0
  },
  "n-shot": {
    "summarization_": 0
  },
  "higher_is_better": {
    "egyptian_summarization": {
      "rouge1": true,
      "rouge2": true,
      "rougeL": true,
      "rougeLsum": true,
      "bert": true,
      "chrf": true
    },
    "summarization_": {
      "rouge1": true,
      "rouge2": true,
      "rougeL": true,
      "rougeLsum": true,
      "bert": true,
      "chrf": true
    }
  },
  "n-samples": {
    "summarization_": {
      "original": 1378,
      "effective": 2
    }
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=habdine/Nile-Chat-4B-Ar-ep1,parallelize=True,trust_remote_code=True",
    "batch_size": "1",
    "batch_sizes": [],
    "device": "mps",
    "use_cache": "False",
    "limit": 2.0,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "2b6c13e2",
  "date": 1744623539.996725,
  "pretty_env_info": "PyTorch version: 2.5.1\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: macOS 14.6 (arm64)\nGCC version: Could not collect\nClang version: 15.0.0 (clang-1500.3.9.4)\nCMake version: version 3.31.6\nLibc version: N/A\n\nPython version: 3.10.12 (main, Nov 22 2024, 15:03:00) [Clang 15.0.0 (clang-1500.3.9.4)] (64-bit runtime)\nPython platform: macOS-14.6-arm64-arm-64bit\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nApple M3 Max\n\nVersions of relevant libraries:\n[pip3] mypy-extensions==1.0.0\n[pip3] numpy==1.26.4\n[pip3] optree==0.13.1\n[pip3] torch==2.5.1\n[pip3] torchaudio==2.5.1\n[pip3] torchmetrics==1.6.0\n[pip3] torchvision==0.20.1\n[conda] numpy                     1.26.4          py312h7f4fdc5_0  \n[conda] numpy-base                1.26.4          py312he047099_0  \n[conda] numpydoc                  1.7.0           py312hca03da5_0  \n[conda] optree                    0.14.1                   pypi_0    pypi\n[conda] torch                     2.6.0                    pypi_0    pypi",
  "transformers_version": "4.51.2",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<pad>",
    "0"
  ],
  "tokenizer_eos_token": [
    "<eos>",
    "1"
  ],
  "tokenizer_bos_token": [
    "<bos>",
    "2"
  ],
  "eot_token_id": 1,
  "max_length": 131072,
  "task_hashes": {
    "summarization_": "85e3c97009d2a069f31ae51f53115750ef1b02a3e66b9f91f14629317bfb403c"
  },
  "model_source": "hf",
  "model_name": "habdine/Nile-Chat-4B-Ar-ep1",
  "model_name_sanitized": "habdine__Nile-Chat-4B-Ar-ep1",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": "{{ bos_token }}\n{%- if messages[0]['role'] == 'system' -%}\n    {%- if messages[0]['content'] is string -%}\n        {%- set first_user_prefix = messages[0]['content'] + '\n\n' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n\n' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message['role'] == 'assistant') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message['role'] -%}\n    {%- endif -%}\n    {{ '<start_of_turn>' + role + '\n' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message['content'] is string -%}\n        {{ message['content'] | trim }}\n    {%- elif message['content'] is iterable -%}\n        {%- for item in message['content'] -%}\n            {%- if item['type'] == 'image' -%}\n                {{ '<start_of_image>' }}\n            {%- elif item['type'] == 'text' -%}\n                {{ item['text'] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ '<end_of_turn>\n' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{'<start_of_turn>model\n'}}\n{%- endif -%}\n",
  "chat_template_sha": "7de1c58e208eda46e9c7f86397df37ec49883aeece39fb961e0a6b24088dd3c4",
  "start_time": 1148399.848901541,
  "end_time": 1148479.256789333,
  "total_evaluation_time_seconds": "79.40788779198192"
}